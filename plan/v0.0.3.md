# VSCode Semantic Search Extension - v0.0.3

## Overview

Simplify architecture by **replacing ChromaDB with DuckDB VSS extension** for vector storage. Use **Transformers.js** directly for embeddings. This eliminates the need for external processes and platform-specific binaries.

## Goals

- **Consolidate to single database (DuckDB)** - Remove ChromaDB dependency
- **Use DuckDB VSS extension** for HNSW vector similarity search
- **Use Transformers.js directly** for embedding generation
- **Simplify deployment** - No external processes or binaries needed
- **Maintain feature parity** with v0.0.2

## Architecture Changes

### Before (v0.0.2)
```
┌─────────────────┐     ┌─────────────────┐
│  Transformers   │     │    ChromaDB     │
│  (via chromadb- │     │    Server       │
│  default-embed) │     │  (executable)   │
└────────┬────────┘     └────────┬────────┘
         │                       │
         │ embeddings            │ vector storage
         │                       │
         └───────────┬───────────┘
                     │
              ┌──────┴──────┐
              │   DuckDB    │
              │ (metadata)  │
              └─────────────┘
```

### After (v0.0.3)
```
┌─────────────────┐
│  Transformers.js│
│  (pipeline API) │
└────────┬────────┘
         │ embeddings
         │
   ┌─────┴─────┐
   │  DuckDB   │
   │  + VSS    │
   │ extension │
   └───────────┘
   (vectors + metadata)
```

## Benefits

1. **Simpler deployment** - No ChromaDB binaries to bundle
2. **Single database** - All data in one DuckDB file  
3. **Better portability** - DuckDB works everywhere Node.js runs
4. **Reduced memory** - No separate server process
5. **Atomic operations** - Vectors and metadata in same transaction
6. **Easier debugging** - Can query vectors with SQL

---

## Embedding Model: all-MiniLM-L6-v2

### Why This Model?

| Property | Value |
|----------|-------|
| Model | `onnx-community/embeddinggemma-300m-ONNX` |
| Output Dimensions | **768** |
| Max Sequence Length | 256 tokens |
| Model Size | ~12MB (q4) / ~23MB (q8) / ~90MB (fp32) |
| Type | Sentence Transformer (BERT-based) |
| License | Apache 2.0 |

**Advantages:**
- Small and fast - good for local/embedded use
- Well-suited for code/text similarity
- ONNX format ready for Transformers.js
- Good balance of quality vs speed
- No prefix required (unlike some newer models)

### Alternative Models (Future)

| Model | Dims | Size | Notes |
|-------|------|------|-------|
| `onnx-community/embeddinggemma-300m-ONNX` | 768 | 23MB | Default, fast, no prefix |
| `Xenova/all-MiniLM-L12-v2` | 768 | 33MB | Better quality |
| `Xenova/bge-small-en-v1.5` | 768 | 33MB | State-of-art small |
| `onnx-community/embeddinggemma-300m-ONNX` | 1024 | ~300MB | High quality, needs prefix |

---

## Transformers.js Usage

### Package

Install `@huggingface/transformers` directly (NOT `chromadb-default-embed`):

```bash
npm install @huggingface/transformers
```

```typescript
import { AutoModel, AutoTokenizer } from "@huggingface/transformers";
```

### AutoModel + AutoTokenizer API

This approach gives more control than `pipeline()` and works better for embedding models:

```typescript
import { AutoModel, AutoTokenizer, Tensor } from "@huggingface/transformers";

// Load model and tokenizer
const modelId = "onnx-community/embeddinggemma-300m-ONNX";

const tokenizer = await AutoTokenizer.from_pretrained(modelId);
const model = await AutoModel.from_pretrained(modelId, {
    dtype: "q4",  // Options: "fp32" | "q8" | "q4" (q4 = smallest)
});

// Tokenize input
const texts = ["function add(a, b) { return a + b; }"];
const inputs = await tokenizer(texts, { 
    padding: true, 
    truncation: true,
    max_length: 256 
});

// Run inference
const outputs = await model(inputs);
// outputs contains: last_hidden_state, pooler_output, etc.
```

### Mean Pooling Implementation

For sentence embeddings, we need to pool the token embeddings:

```typescript
/**
 * Mean pooling - average all token embeddings, weighted by attention mask
 */
function meanPooling(
    lastHiddenState: Tensor,  // [batch, seq_len, hidden_dim]
    attentionMask: Tensor      // [batch, seq_len]
): Tensor {
    // Expand attention mask to match hidden state dimensions
    const maskExpanded = attentionMask.unsqueeze(-1).expand(lastHiddenState.dims);
    
    // Sum of embeddings weighted by mask
    const sumEmbeddings = lastHiddenState.mul(maskExpanded).sum(1);
    
    // Sum of mask (to get count of real tokens)
    const sumMask = maskExpanded.sum(1).clamp(1e-9, Infinity);
    
    // Mean = sum / count
    return sumEmbeddings.div(sumMask);
}

/**
 * L2 normalize embeddings (required for cosine similarity)
 */
function normalize(tensor: Tensor): Tensor {
    const norm = tensor.norm(2, -1, true);
    return tensor.div(norm);
}
```

### Complete Embedding Function

```typescript
async function embed(texts: string[]): Promise<number[][]> {
    // Tokenize
    const inputs = await tokenizer(texts, {
        padding: true,
        truncation: true,
        max_length: 256
    });
    
    // Run model
    const { last_hidden_state } = await model(inputs);
    
    // Mean pooling
    const pooled = meanPooling(last_hidden_state, inputs.attention_mask);
    
    // Normalize for cosine similarity
    const normalized = normalize(pooled);
    
    // Convert to array
    return normalized.tolist();
}

// Usage
const embeddings = await embed([
    "function add(a, b) { return a + b; }",
    "class Calculator { constructor() {} }"
]);
// embeddings[0] = [0.023, -0.045, ...] (768 dimensions)
// embeddings[1] = [0.012, 0.067, ...] (768 dimensions)
```

### Model Loading Options

```typescript
const model = await AutoModel.from_pretrained(modelId, {
    dtype: "q8",           // Quantization: "fp32" | "q8" | "q4"
    device: "cpu",         // Device: "cpu" | "gpu" | "wasm"
    progress_callback: (progress) => {
        // progress = { status, file, loaded, total }
        console.log(`${progress.status}: ${progress.loaded}/${progress.total}`);
    },
    cache_dir: "/path/to/cache",  // Custom cache directory
});
```

### Caching

Models are cached automatically:
- **Node.js**: `~/.cache/huggingface/hub/`
- **Custom**: Set via `cache_dir` option or `HF_HOME` env variable

```typescript
// Use extension storage for cache
const cacheDir = path.join(context.globalStorageUri.fsPath, 'models');

const model = await AutoModel.from_pretrained(modelId, {
    cache_dir: cacheDir
});
```

---

## DuckDB VSS Extension

### Installation & Loading

```sql
-- Install extension (downloads from DuckDB extension repo)
INSTALL vss;

-- Load extension into current session
LOAD vss;

-- Enable experimental persistence (required for disk-backed DBs)
SET hnsw_enable_experimental_persistence = true;
```

### Vector Column Type

```sql
-- FLOAT[N] is fixed-size array, N = embedding dimensions
CREATE TABLE code_chunks (
    chunk_id VARCHAR PRIMARY KEY,
    content TEXT NOT NULL,
    embedding FLOAT[768]  -- 768 dimensions for all-MiniLM-L6-v2
);
```

### Creating HNSW Index

```sql
-- Create HNSW index with cosine distance metric
CREATE INDEX idx_embedding 
ON code_chunks 
USING HNSW (embedding)
WITH (
    metric = 'cosine',       -- 'l2sq' (euclidean), 'cosine', 'ip' (inner product)
    ef_construction = 128,   -- Build-time accuracy (higher = better, slower)
    ef_search = 64,          -- Search-time accuracy (higher = better, slower)
    M = 16                   -- Max neighbors per node (higher = better, more memory)
);
```

### Similarity Search

```sql
-- Vector similarity search using array_cosine_distance
SELECT 
    chunk_id,
    content,
    array_cosine_distance(embedding, $query_embedding::FLOAT[768]) AS distance
FROM code_chunks
ORDER BY array_cosine_distance(embedding, $query_embedding::FLOAT[768])
LIMIT 10;
```

### Distance Functions

| Metric | Function | Index Option | Notes |
|--------|----------|--------------|-------|
| Euclidean (L2²) | `array_distance(a, b)` | `l2sq` | Default |
| Cosine | `array_cosine_distance(a, b)` | `cosine` | **Recommended for text** |
| Inner Product | `array_negative_inner_product(a, b)` | `ip` | For normalized vectors |

### Important Notes

1. **Persistence is Experimental**: Enable with `SET hnsw_enable_experimental_persistence = true`
2. **Memory**: Index must fit in RAM
3. **Deletes**: Deleted rows are marked, not removed. Use `PRAGMA hnsw_compact_index('idx_name')` to clean up
4. **Updates**: Update = Delete + Insert. May cause index fragmentation.

---

## Technical Design

### 1. Database Schema

```sql
-- Install and load VSS extension
INSTALL vss;
LOAD vss;

-- Enable experimental persistence for disk-backed databases
SET hnsw_enable_experimental_persistence = true;

-- Indexed files metadata
CREATE TABLE IF NOT EXISTS indexed_files (
    file_id VARCHAR PRIMARY KEY,
    file_path VARCHAR NOT NULL,
    workspace_path VARCHAR NOT NULL,
    md5_hash VARCHAR NOT NULL,
    last_indexed_at BIGINT NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_workspace_path ON indexed_files(workspace_path);
CREATE INDEX IF NOT EXISTS idx_file_path ON indexed_files(file_path);

-- Code chunks with embeddings
CREATE TABLE IF NOT EXISTS code_chunks (
    chunk_id VARCHAR PRIMARY KEY,
    file_id VARCHAR NOT NULL,
    file_path VARCHAR NOT NULL,
    workspace_path VARCHAR NOT NULL,
    content TEXT NOT NULL,
    line_start INTEGER NOT NULL,
    line_end INTEGER NOT NULL,
    language VARCHAR,
    embedding FLOAT[768],  -- all-MiniLM-L6-v2 produces 768-dim vectors
    created_at BIGINT NOT NULL
);

-- HNSW index for fast similarity search (cosine distance)
CREATE INDEX IF NOT EXISTS idx_chunks_embedding 
ON code_chunks 
USING HNSW (embedding)
WITH (metric = 'cosine');

-- Regular indexes for filtering
CREATE INDEX IF NOT EXISTS idx_chunks_file ON code_chunks(file_id);
CREATE INDEX IF NOT EXISTS idx_chunks_workspace ON code_chunks(workspace_path);
```

### 2. EmbeddingService Implementation

```typescript
// src/services/embeddingService.ts
import * as vscode from 'vscode';
import * as path from 'path';

// Types for @huggingface/transformers
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type HFTokenizer = any;
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type HFModel = any;
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type Tensor = any;

export interface EmbeddingProgress {
    status: string;
    file?: string;
    loaded?: number;
    total?: number;
}

export class EmbeddingService {
    private tokenizer: HFTokenizer | null = null;
    private model: HFModel | null = null;
    private modelId = 'onnx-community/embeddinggemma-300m-ONNX';
    private dimensions = 768;
    private maxLength = 256;
    private initPromise: Promise<void> | null = null;
    
    constructor(private context: vscode.ExtensionContext) {}
    
    /**
     * Initialize the embedding model and tokenizer
     */
    async initialize(
        onProgress?: (progress: EmbeddingProgress) => void
    ): Promise<void> {
        if (this.model && this.tokenizer) return;
        if (this.initPromise) return this.initPromise;
        
        this.initPromise = this.loadModel(onProgress);
        await this.initPromise;
    }
    
    private async loadModel(
        onProgress?: (progress: EmbeddingProgress) => void
    ): Promise<void> {
        const { AutoModel, AutoTokenizer } = await import('@huggingface/transformers');
        
        // Custom cache directory in extension storage
        const cacheDir = path.join(
            this.context.globalStorageUri.fsPath,
            'models'
        );
        
        const options = {
            dtype: 'q4' as const,  // Use smallest quantized model
            cache_dir: cacheDir,
            progress_callback: onProgress
        };
        
        // Load tokenizer and model in parallel
        const [tokenizer, model] = await Promise.all([
            AutoTokenizer.from_pretrained(this.modelId, options),
            AutoModel.from_pretrained(this.modelId, options)
        ]);
        
        this.tokenizer = tokenizer;
        this.model = model;
    }
    
    /**
     * Mean pooling - average token embeddings weighted by attention mask
     */
    private meanPooling(lastHiddenState: Tensor, attentionMask: Tensor): Tensor {
        // Get dimensions
        const [batchSize, seqLen, hiddenDim] = lastHiddenState.dims;
        
        // Expand attention mask to match hidden state: [batch, seq] -> [batch, seq, hidden]
        const maskExpanded = attentionMask
            .unsqueeze(-1)
            .expand([batchSize, seqLen, hiddenDim]);
        
        // Multiply hidden states by mask and sum along sequence dimension
        const sumEmbeddings = lastHiddenState.mul(maskExpanded).sum(1);
        
        // Sum mask along sequence dimension
        const sumMask = maskExpanded.sum(1).clamp(1e-9, Infinity);
        
        // Compute mean
        return sumEmbeddings.div(sumMask);
    }
    
    /**
     * L2 normalize embeddings
     */
    private normalize(tensor: Tensor): Tensor {
        const norm = tensor.norm(2, -1, true);
        return tensor.div(norm);
    }
    
    /**
     * Generate embedding for a single text
     */
    async embed(text: string): Promise<number[]> {
        const embeddings = await this.embedBatch([text]);
        return embeddings[0];
    }
    
    /**
     * Generate embeddings for multiple texts
     */
    async embedBatch(texts: string[]): Promise<number[][]> {
        if (!this.tokenizer || !this.model) {
            throw new Error('EmbeddingService not initialized');
        }
        
        if (texts.length === 0) return [];
        
        // Tokenize
        const inputs = await this.tokenizer(texts, {
            padding: true,
            truncation: true,
            max_length: this.maxLength
        });
        
        // Run model inference
        const outputs = await this.model(inputs);
        
        // Mean pooling over token embeddings
        const pooled = this.meanPooling(outputs.last_hidden_state, inputs.attention_mask);
        
        // L2 normalize for cosine similarity
        const normalized = this.normalize(pooled);
        
        // Convert tensor to array
        return normalized.tolist();
    }
    
    /**
     * Get embedding dimensions
     */
    getDimensions(): number {
        return this.dimensions;
    }
    
    /**
     * Get model name
     */
    getModelId(): string {
        return this.modelId;
    }
    
    /**
     * Check if service is initialized
     */
    isInitialized(): boolean {
        return this.model !== null && this.tokenizer !== null;
    }
}
```

### 3. VectorDbService Implementation

```typescript
// src/services/vectorDbService.ts
import * as path from 'path';
import * as fs from 'fs';
import { EmbeddingService } from './embeddingService';

interface CodeChunk {
    chunkId: string;
    fileId: string;
    filePath: string;
    workspacePath: string;
    content: string;
    lineStart: number;
    lineEnd: number;
    language?: string;
}

interface SearchResult {
    chunkId: string;
    filePath: string;
    content: string;
    lineStart: number;
    lineEnd: number;
    distance: number;
    score: number;  // 1 - distance (for cosine)
}

export class VectorDbService {
    private instance: any = null;
    private connection: any = null;
    private storagePath: string;
    private dbPath: string;
    private initialized = false;
    private dimensions: number;
    
    constructor(
        storagePath: string,
        private embeddingService: EmbeddingService
    ) {
        this.storagePath = storagePath;
        this.dbPath = path.join(storagePath, 'semantic_search.duckdb');
        this.dimensions = embeddingService.getDimensions();
    }
    
    async initialize(): Promise<void> {
        if (this.initialized) return;
        
        // Ensure storage directory exists
        if (!fs.existsSync(this.storagePath)) {
            fs.mkdirSync(this.storagePath, { recursive: true });
        }
        
        const { DuckDBInstance } = require('@duckdb/node-api');
        
        this.instance = await DuckDBInstance.create(this.dbPath);
        this.connection = await this.instance.connect();
        
        // Install and load VSS extension
        await this.connection.run('INSTALL vss');
        await this.connection.run('LOAD vss');
        await this.connection.run('SET hnsw_enable_experimental_persistence = true');
        
        // Create schema
        await this.createSchema();
        
        this.initialized = true;
    }
    
    private async createSchema(): Promise<void> {
        // Create indexed_files table
        await this.connection.run(`
            CREATE TABLE IF NOT EXISTS indexed_files (
                file_id VARCHAR PRIMARY KEY,
                file_path VARCHAR NOT NULL,
                workspace_path VARCHAR NOT NULL,
                md5_hash VARCHAR NOT NULL,
                last_indexed_at BIGINT NOT NULL
            )
        `);
        
        // Create code_chunks table with embedding column
        await this.connection.run(`
            CREATE TABLE IF NOT EXISTS code_chunks (
                chunk_id VARCHAR PRIMARY KEY,
                file_id VARCHAR NOT NULL,
                file_path VARCHAR NOT NULL,
                workspace_path VARCHAR NOT NULL,
                content TEXT NOT NULL,
                line_start INTEGER NOT NULL,
                line_end INTEGER NOT NULL,
                language VARCHAR,
                embedding FLOAT[${this.dimensions}],
                created_at BIGINT NOT NULL
            )
        `);
        
        // Create HNSW index for vector search
        // Note: This may fail if index already exists, which is fine
        try {
            await this.connection.run(`
                CREATE INDEX idx_chunks_embedding 
                ON code_chunks 
                USING HNSW (embedding)
                WITH (metric = 'cosine')
            `);
        } catch (e) {
            // Index may already exist
        }
        
        // Create regular indexes
        await this.connection.run(`
            CREATE INDEX IF NOT EXISTS idx_workspace_path ON indexed_files(workspace_path)
        `);
        await this.connection.run(`
            CREATE INDEX IF NOT EXISTS idx_chunks_file ON code_chunks(file_id)
        `);
    }
    
    /**
     * Add a code chunk with its embedding
     */
    async addChunk(chunk: CodeChunk): Promise<void> {
        // Generate embedding
        const embedding = await this.embeddingService.embed(chunk.content);
        
        // Format embedding as DuckDB array literal
        const embeddingStr = `[${embedding.join(',')}]::FLOAT[${this.dimensions}]`;
        
        const sql = `
            INSERT INTO code_chunks 
            (chunk_id, file_id, file_path, workspace_path, content, 
             line_start, line_end, language, embedding, created_at)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, ${embeddingStr}, $9)
            ON CONFLICT (chunk_id) DO UPDATE SET
                content = excluded.content,
                line_start = excluded.line_start,
                line_end = excluded.line_end,
                embedding = excluded.embedding,
                created_at = excluded.created_at
        `;
        
        const stmt = await this.connection.prepare(sql);
        stmt.bindValue(1, chunk.chunkId);
        stmt.bindValue(2, chunk.fileId);
        stmt.bindValue(3, chunk.filePath);
        stmt.bindValue(4, chunk.workspacePath);
        stmt.bindValue(5, chunk.content);
        stmt.bindValue(6, chunk.lineStart);
        stmt.bindValue(7, chunk.lineEnd);
        stmt.bindValue(8, chunk.language || null);
        stmt.bindValue(9, Date.now());
        await stmt.run();
    }
    
    /**
     * Search for similar code chunks
     */
    async search(
        query: string, 
        workspacePath?: string,
        limit: number = 10
    ): Promise<SearchResult[]> {
        // Generate query embedding
        const queryEmbedding = await this.embeddingService.embed(query);
        const embeddingStr = `[${queryEmbedding.join(',')}]::FLOAT[${this.dimensions}]`;
        
        let sql = `
            SELECT 
                chunk_id,
                file_path,
                content,
                line_start,
                line_end,
                array_cosine_distance(embedding, ${embeddingStr}) AS distance
            FROM code_chunks
        `;
        
        if (workspacePath) {
            sql += ` WHERE workspace_path = '${workspacePath.replace(/'/g, "''")}'`;
        }
        
        sql += `
            ORDER BY array_cosine_distance(embedding, ${embeddingStr})
            LIMIT ${limit}
        `;
        
        const result = await this.connection.run(sql);
        const rows = await result.getRows();
        const columnNames = result.columnNames();
        
        return rows.map((row: any[]) => {
            const obj: any = {};
            columnNames.forEach((col: string, i: number) => {
                obj[col] = row[i];
            });
            return {
                chunkId: obj.chunk_id,
                filePath: obj.file_path,
                content: obj.content,
                lineStart: obj.line_start,
                lineEnd: obj.line_end,
                distance: obj.distance,
                score: 1 - obj.distance  // Convert distance to similarity score
            };
        });
    }
    
    /**
     * Delete all chunks for a file
     */
    async deleteFileChunks(fileId: string): Promise<void> {
        await this.connection.run(
            `DELETE FROM code_chunks WHERE file_id = '${fileId.replace(/'/g, "''")}'`
        );
    }
    
    /**
     * Compact the HNSW index (removes deleted entries)
     */
    async compactIndex(): Promise<void> {
        await this.connection.run(`PRAGMA hnsw_compact_index('idx_chunks_embedding')`);
    }
    
    /**
     * Close database connection
     */
    async close(): Promise<void> {
        if (this.connection) {
            this.connection.closeSync();
            this.connection = null;
        }
        if (this.instance) {
            this.instance.closeSync();
            this.instance = null;
        }
        this.initialized = false;
    }
}
```

---

## Implementation Tasks

### Phase 1: Core Infrastructure

- [ ] **1.1 Create EmbeddingService** (`src/services/embeddingService.ts`)
  - [ ] Use `@huggingface/transformers` with `AutoModel` + `AutoTokenizer`
  - [ ] Use `onnx-community/embeddinggemma-300m-ONNX` model (768 dimensions)
  - [ ] Implement mean pooling over `last_hidden_state`
  - [ ] Implement L2 normalization for cosine similarity
  - [ ] Implement `embed(text)` and `embedBatch(texts)` methods
  - [ ] Add progress callback for model download
  - [ ] Cache models in extension globalStorage
  - [ ] Use `dtype: "q4"` for smallest quantized model

- [ ] **1.2 Update DuckDB Service for VSS** (`src/services/vectorDbService.ts`)
  - [ ] Install and load VSS extension on init
  - [ ] Enable experimental persistence
  - [ ] Create `code_chunks` table with `FLOAT[768]` embedding column
  - [ ] Create HNSW index with `metric = 'cosine'`
  - [ ] Implement `addChunk()` with embedding generation
  - [ ] Implement `search()` using `array_cosine_distance()`
  - [ ] Implement `compactIndex()` for maintenance

- [ ] **1.3 Remove ChromaDB Dependencies**
  - [ ] Delete `src/services/chromaService.ts`
  - [ ] Delete `src/services/chromaProcessManager.ts`
  - [ ] Remove ChromaDB settings from `package.json` contributes
  - [ ] Remove `chromadb` and `chromadb-default-embed` from dependencies
  - [ ] Add `@huggingface/transformers` to dependencies
  - [ ] Delete `bin/` directory with Chroma executables

### Phase 2: Service Integration

- [ ] **2.1 Update IndexingService**
  - [ ] Inject EmbeddingService and VectorDbService
  - [ ] Use `addChunk()` for each code chunk
  - [ ] Update progress reporting for embedding generation

- [ ] **2.2 Update SearchService**
  - [ ] Use `VectorDbService.search()` 
  - [ ] Maintain same `SearchResult` interface
  - [ ] Add workspace filtering support

- [ ] **2.3 Update Extension Activation** (`src/extension.ts`)
  - [ ] Initialize EmbeddingService with progress UI
  - [ ] Initialize VectorDbService with EmbeddingService
  - [ ] Remove Chroma-related initialization
  - [ ] Add model download progress notification

### Phase 3: UI Updates

- [ ] **3.1 Update Status Bar**
  - [ ] Remove Chroma server status indicator
  - [ ] Add model loading status
  - [ ] Show "Embedding model ready" when initialized

- [ ] **3.2 Update Settings** (`package.json`)
  - [ ] Remove `semanticSearch.chroma.*` settings
  - [ ] Add `semanticSearch.embeddingModel` (future, default to all-MiniLM-L6-v2)
  - [ ] Keep existing indexing settings

### Phase 4: Testing & Cleanup

- [ ] **4.1 Testing**
  - [ ] Test embedding generation
  - [ ] Test vector search accuracy
  - [ ] Test index persistence across restarts
  - [ ] Test large workspace indexing

- [ ] **4.2 Documentation**
  - [ ] Update README.md
  - [ ] Update CHANGELOG.md

---

## Dependencies Changes

### Remove from `package.json`
```json
{
  "chromadb": "^1.10.5",
  "chromadb-default-embed": "^2.13.2"
}
```

### Add
```json
{
  "@huggingface/transformers": "^3.0.0"
}
```

### Keep
```json
{
  "@duckdb/node-api": "^1.4.2-r.1",
  "@duckdb/node-bindings": "^1.4.2-r.1"
}
```

---

## Migration Path

For users upgrading from v0.0.2:

1. **Automatic re-indexing** required (new storage format)
2. **Settings migration** - Deprecated Chroma settings ignored
3. **First run** - Model download (~23MB) with progress indicator

---

## Performance Expectations

| Operation | Expected Time |
|-----------|---------------|
| Model load (first) | 5-30s (download) |
| Model load (cached) | 1-3s |
| Single embedding | 10-50ms |
| Batch embedding (10) | 50-200ms |
| Vector search (10k chunks) | 10-50ms |
| Full index (1000 files) | 2-5 minutes |

---

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| VSS persistence experimental | Implement index rebuild on corruption detection |
| Model download on first use | Show progress notification, allow offline model |
| Large embedding memory | Use quantized model, batch processing |
| Network required for model | Bundle model or provide offline setup guide |

---

## Success Criteria

- [ ] Extension activates successfully
- [ ] Search returns relevant results
- [ ] No external process dependencies
- [ ] Model cached after first download
- [ ] Index persists across VS Code restarts
